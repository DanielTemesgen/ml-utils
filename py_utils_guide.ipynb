{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# py-utils guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's import py_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the function below gives us the ability to hide code cells in jupyter notebook. <br>\n",
    "This can be helpful when presenting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "    code_show=false; \n",
       "    function code_toggle() {\n",
       "     if (code_show){\n",
       "     $('div.input').hide();\n",
       "     } else {\n",
       "     $('div.input').show();\n",
       "     }\n",
       "     code_show = !code_show\n",
       "    } \n",
       "    $( document ).ready(code_toggle);\n",
       "    </script>\n",
       "    The code for this jupyter notebook has been hidden by default for easier reading.\n",
       "    To toggle on/off the code, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "py_utils.hide_code_cells()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore how to use the model utils let's quickly make a test model. <br>\n",
    "Firstly, import some more packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import make_classification, load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a fake numeric dataset for the purpose of our examples.\n",
    "\n",
    "We'll do this by using scikit-learn's helpful `make_classification` function.\n",
    "\n",
    "<mark>You can 'unpack' arguments into a function via a dictionary using the `**` notation (see below).</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_classification_dict = {'n_samples': 100000, 'n_features': 50}\n",
    "\n",
    "sample_data =  make_classification(**make_classification_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.819823</td>\n",
       "      <td>0.097565</td>\n",
       "      <td>-1.757625</td>\n",
       "      <td>1.227868</td>\n",
       "      <td>0.131023</td>\n",
       "      <td>0.426952</td>\n",
       "      <td>-0.901286</td>\n",
       "      <td>-0.146374</td>\n",
       "      <td>0.144019</td>\n",
       "      <td>-1.730063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.608063</td>\n",
       "      <td>0.373808</td>\n",
       "      <td>0.212106</td>\n",
       "      <td>-0.480824</td>\n",
       "      <td>0.306803</td>\n",
       "      <td>0.541224</td>\n",
       "      <td>1.113348</td>\n",
       "      <td>0.371633</td>\n",
       "      <td>-1.794812</td>\n",
       "      <td>-0.819101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.522197</td>\n",
       "      <td>0.151296</td>\n",
       "      <td>1.289466</td>\n",
       "      <td>-1.160126</td>\n",
       "      <td>0.129245</td>\n",
       "      <td>0.599367</td>\n",
       "      <td>0.100735</td>\n",
       "      <td>-0.569687</td>\n",
       "      <td>-1.026696</td>\n",
       "      <td>1.189401</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.801043</td>\n",
       "      <td>0.519837</td>\n",
       "      <td>1.985378</td>\n",
       "      <td>0.870569</td>\n",
       "      <td>-0.361663</td>\n",
       "      <td>0.991291</td>\n",
       "      <td>-0.187126</td>\n",
       "      <td>-0.725366</td>\n",
       "      <td>0.151034</td>\n",
       "      <td>-1.352145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.997706</td>\n",
       "      <td>-0.353547</td>\n",
       "      <td>0.878651</td>\n",
       "      <td>-0.784361</td>\n",
       "      <td>-1.102551</td>\n",
       "      <td>0.035339</td>\n",
       "      <td>0.530357</td>\n",
       "      <td>-0.415905</td>\n",
       "      <td>0.624581</td>\n",
       "      <td>-1.543561</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.413638</td>\n",
       "      <td>-1.951667</td>\n",
       "      <td>-0.192065</td>\n",
       "      <td>-0.166177</td>\n",
       "      <td>-0.111217</td>\n",
       "      <td>-0.077342</td>\n",
       "      <td>-2.155042</td>\n",
       "      <td>0.265217</td>\n",
       "      <td>0.731313</td>\n",
       "      <td>2.069130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.511898</td>\n",
       "      <td>0.243341</td>\n",
       "      <td>2.651493</td>\n",
       "      <td>-0.827056</td>\n",
       "      <td>0.114937</td>\n",
       "      <td>-0.061088</td>\n",
       "      <td>0.278560</td>\n",
       "      <td>0.504284</td>\n",
       "      <td>0.840692</td>\n",
       "      <td>-0.881697</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.079124</td>\n",
       "      <td>0.289213</td>\n",
       "      <td>0.209501</td>\n",
       "      <td>-0.375945</td>\n",
       "      <td>0.460960</td>\n",
       "      <td>0.409314</td>\n",
       "      <td>0.044036</td>\n",
       "      <td>-2.207479</td>\n",
       "      <td>0.359103</td>\n",
       "      <td>0.175793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.591561</td>\n",
       "      <td>-1.614747</td>\n",
       "      <td>-0.991083</td>\n",
       "      <td>-0.850550</td>\n",
       "      <td>-0.053781</td>\n",
       "      <td>0.204666</td>\n",
       "      <td>-0.202397</td>\n",
       "      <td>-0.203307</td>\n",
       "      <td>0.473752</td>\n",
       "      <td>1.008277</td>\n",
       "      <td>...</td>\n",
       "      <td>0.295646</td>\n",
       "      <td>0.734089</td>\n",
       "      <td>1.383470</td>\n",
       "      <td>-0.339204</td>\n",
       "      <td>1.356450</td>\n",
       "      <td>0.588089</td>\n",
       "      <td>0.065331</td>\n",
       "      <td>-1.344490</td>\n",
       "      <td>-1.292817</td>\n",
       "      <td>0.478965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  1.819823  0.097565 -1.757625  1.227868  0.131023  0.426952 -0.901286   \n",
       "1  0.522197  0.151296  1.289466 -1.160126  0.129245  0.599367  0.100735   \n",
       "2 -1.997706 -0.353547  0.878651 -0.784361 -1.102551  0.035339  0.530357   \n",
       "3  0.511898  0.243341  2.651493 -0.827056  0.114937 -0.061088  0.278560   \n",
       "4  0.591561 -1.614747 -0.991083 -0.850550 -0.053781  0.204666 -0.202397   \n",
       "\n",
       "         7         8         9   ...        40        41        42        43  \\\n",
       "0 -0.146374  0.144019 -1.730063  ...  0.608063  0.373808  0.212106 -0.480824   \n",
       "1 -0.569687 -1.026696  1.189401  ... -0.801043  0.519837  1.985378  0.870569   \n",
       "2 -0.415905  0.624581 -1.543561  ... -1.413638 -1.951667 -0.192065 -0.166177   \n",
       "3  0.504284  0.840692 -0.881697  ... -2.079124  0.289213  0.209501 -0.375945   \n",
       "4 -0.203307  0.473752  1.008277  ...  0.295646  0.734089  1.383470 -0.339204   \n",
       "\n",
       "         44        45        46        47        48        49  \n",
       "0  0.306803  0.541224  1.113348  0.371633 -1.794812 -0.819101  \n",
       "1 -0.361663  0.991291 -0.187126 -0.725366  0.151034 -1.352145  \n",
       "2 -0.111217 -0.077342 -2.155042  0.265217  0.731313  2.069130  \n",
       "3  0.460960  0.409314  0.044036 -2.207479  0.359103  0.175793  \n",
       "4  1.356450  0.588089  0.065331 -1.344490 -1.292817  0.478965  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 651,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.DataFrame(sample_data[0]).fillna(np.median)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our y: <br>\n",
    "<mark>You can continue code onto the next line with `\\` (see below)</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  1\n",
       "1  1\n",
       "2  0\n",
       "3  0\n",
       "4  1"
      ]
     },
     "execution_count": 652,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.Series(sample_data[1])\\\n",
    ".to_frame()\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now let's say we want to apply a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 654,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(solver='lbfgs')\n",
    "logreg.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90484"
      ]
     },
     "execution_count": 655,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the model, the function assumes that we have a 'Models' subfolder so we have one created here. <br>\n",
    "We need to define the following things for this function:\n",
    "* `model_name` = the root name of the model we want to save.\n",
    "* `model_var` = the model variable we want to save.\n",
    "* `subfolder` = optional subfolder we want to use, we don't need it here so let's leave it as an empty string.\n",
    "* `wd` = working directory, the main working directory of our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_utils.dump_diff_model(model_name='my_model', model_var=logreg, subfolder='', wd = os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see this file, which is saved below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my_model_2019-07-22 23:50:11.724130.joblib']"
      ]
     },
     "execution_count": 658,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in os.listdir(os.path.join(os.getcwd(), 'Models')) if x.endswith('.joblib')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try and save it again and check the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_utils.dump_diff_model(model_name='my_model', model_var=logreg, subfolder='', wd = os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my_model_2019-07-22 23:50:11.724130.joblib']"
      ]
     },
     "execution_count": 660,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in os.listdir(os.path.join(os.getcwd(), 'Models')) if x.endswith('.joblib')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that nothing has saved! <br>\n",
    "This is because we are trying to save the same model twice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try and save a different model with the same name 'my_model'. <br>\n",
    "Let's say we want to change Logistic Regression solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='saga',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 661,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(solver='saga')\n",
    "logreg.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90484"
      ]
     },
     "execution_count": 662,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now changed the solver from 'lbfgs' to 'saga', now let's try and save this different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_utils.dump_diff_model(model_name='my_model', model_var=logreg, subfolder='', wd = os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my_model_2019-07-22 23:50:11.724130.joblib',\n",
       " 'my_model_2019-07-22 23:50:14.303373.joblib']"
      ]
     },
     "execution_count": 664,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in os.listdir(os.path.join(os.getcwd(), 'Models')) if x.endswith('.joblib')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this new model has been saved. <br>\n",
    "This is because it is different from the most recent version of that model as defined by its timestamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the docstring for this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function dump_diff_model in module py_utils:\n",
      "\n",
      "dump_diff_model(model_name, model_var, wd, subfolder='')\n",
      "    This function will dump your model with a unique timestamp associated with it.\n",
      "    First it will check if the most recent model is the same as the one you're trying to dump, \n",
      "    if this is case then the funciton will do nothing.\n",
      "    \n",
      "    Example:\n",
      "    Let's say you want to dump a model called log_reg. You want to dump this file with the name 'master_model'.\n",
      "    \n",
      "    py_utils.dump_diff_model(name = 'master_model', log_reg, wd = \"/Users/Daniel/Desktop/Projects/First_Project/\")\n",
      "    \n",
      "    Args:\n",
      "    model_mame (str): the name you want to give the model you're dumping.\n",
      "    model_var: the actual model itself.\n",
      "    wd (str): your working directory, the function will look in the /Models subfolder of this working directory.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(py_utils.dump_diff_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know how to save models, let's see how we load them. <br>\n",
    "First let's import out necessary package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use another one of these util functions: most_recent_model() <br>\n",
    "Very simply, it returns the path of the most recent model in a directory (based on the timestamp).\n",
    "\n",
    "Let's see what we define for this function:\n",
    "* `model_name` = the root name of the model we want to save.\n",
    "* `wd` = working directory, the main working directory of our project.\n",
    "* `subfolder` = optional subfolder we want to use, we don't need it here so let's leave it as an empty string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get more help by inspecting the docstring for this function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function most_recent_model in module py_utils:\n",
      "\n",
      "most_recent_model(model_name, wd, subfolder)\n",
      "    This function finds the name of the most recent model from the file directory.\n",
      "    It's a companion function to the dunp_diff_model function.\n",
      "    \n",
      "    Example:\n",
      "    If you had a model called 'log_reg' that was saved at this timestamp: '2019-05-21 12/04/54.815294'.\n",
      "    The filename would be 'log_reg 2019-06-21 12/04/54.875304.joblib' and the root would be 'log_reg'.\n",
      "    \n",
      "    log_reg = load(py_utils.most_recent_model(model_name = 'log_reg', wd = \"/Users/Daniel/Desktop/Projects/First_Project/\"))\n",
      "    \n",
      "    Args:\n",
      "    model_name (str): the name of the model you want to find the most recent version of.\n",
      "    wd (str): your working directory, the function will look in the /Models subfolder of this working directory\n",
      "    \n",
      "    Returns:\n",
      "    most_recent_model_path (str): the filepath of your most recent model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(py_utils.most_recent_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Daniel/Desktop/GitHub_Repos/ml-utils/Models/my_model_2019-07-22 23:50:14.303373.joblib'"
      ]
     },
     "execution_count": 668,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_to_load = py_utils.most_recent_model(model_name = 'my_model', wd = os.getcwd(), subfolder='')\n",
    "model_to_load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the path above. <br>\n",
    "Now let's import this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = joblib.load(model_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='saga',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 670,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see above that we successfully imported this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go through the sensitivity analysis function, this involves: <br>\n",
    "\"... the study of how the uncertainty in the output of a mathematical model or system (numerical or otherwise) can be divided and allocated to different sources of uncertainty in its inputs.\" [Wikipedia](https://en.wikipedia.org/wiki/Sensitivity_analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do sensitivity analysis on the log_reg model, this involves changing each feature by multiple of itself, then scoring the model. <br>\n",
    "The docstring is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sensitivity_analysis in module py_utils:\n",
      "\n",
      "sensitivity_analysis(sens_voi, X_test, y_test, model, multiplier=1.1, scoring_func=<function roc_auc_score at 0x1a209da158>)\n",
      "    This function runs a sensitivity analysis by taking a model then modifying its feature vector in a univariate fashion.\n",
      "    Following this, a scoring function is applied.\n",
      "    When all features are ran through then a summary dataframe is returned.\n",
      "    \n",
      "    Example:\n",
      "    Let's say you have a model called log_reg, you want to do sensitivity analysis on all features in a feature vector called X_train.\n",
      "    You also want to multiply each feature by 1.1 and use roc_auc as your scoring metric.\n",
      "    \n",
      "    py_utils.sensitivity_analysis(\n",
      "        sens_voi = X_train.columns, X_test = X_test, y_test = y_test, model = log_reg, multiplier = 1.1,  scoring_func = sklearn.metrics.roc_auc_score\n",
      "        )\n",
      "        \n",
      "    Args:\n",
      "    sens_voi (list): Sensitivitiy analysis features of interest.\n",
      "    X_test (pd.DataFrame): the feature vector used for scoring.\n",
      "    y_test (pd.Series): the target used for scoring.\n",
      "    model (sklearn.estimator): the model you want to conduct sensitivity analysis on.\n",
      "    multipler (float, int): the number that each feature is multiplied by.\n",
      "    scoring_func (function): the sklearn scoring function you want to use e.g. sklearn.metrics.accuracy_score\n",
      "    \n",
      "    Returns:\n",
      "    sens_df (pd.DataFrame): summary DataFrame showing sensitivity analysis results.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(py_utils.sensitivity_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sensitvitiy analysis returns a DataFrame which has a summary of the analysis, we can also change our scoring function, but here I will roc_auc as this was the metric used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature multiplied by 1.5</th>\n",
       "      <th>Sensitivity roc_auc_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.964411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.964410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.964405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.964412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.964405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.964409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.964396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.964411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.964410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.964409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.964403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.964413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.964379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.964407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.964410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.964410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.964412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.964411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.964403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.964441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.964409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.964409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.964413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.964408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.964391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.964407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.964412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.964408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.964408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.964407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>0.964410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>0.964412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>0.964410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>0.964410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>0.964408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>0.964411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>0.964411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>0.964388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>0.964410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>0.964401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>0.964402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>0.964411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>0.964411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>0.964410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>0.964408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>0.964404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>0.964407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>0.964410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>0.962471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>0.962316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Feature multiplied by 1.5  Sensitivity roc_auc_score\n",
       "0                           0                   0.964411\n",
       "1                           1                   0.964410\n",
       "2                           2                   0.964405\n",
       "3                           3                   0.964412\n",
       "4                           4                   0.964405\n",
       "5                           5                   0.964409\n",
       "6                           6                   0.964396\n",
       "7                           7                   0.964411\n",
       "8                           8                   0.964410\n",
       "9                           9                   0.964409\n",
       "10                         10                   0.964403\n",
       "11                         11                   0.964413\n",
       "12                         12                   0.964379\n",
       "13                         13                   0.964407\n",
       "14                         14                   0.964410\n",
       "15                         15                   0.964410\n",
       "16                         16                   0.964412\n",
       "17                         17                   0.964411\n",
       "18                         18                   0.964403\n",
       "19                         19                   0.964441\n",
       "20                         20                   0.964409\n",
       "21                         21                   0.964409\n",
       "22                         22                   0.964413\n",
       "23                         23                   0.964408\n",
       "24                         24                   0.964391\n",
       "25                         25                   0.964407\n",
       "26                         26                   0.964412\n",
       "27                         27                   0.964408\n",
       "28                         28                   0.964408\n",
       "29                         29                   0.964407\n",
       "30                         30                   0.964410\n",
       "31                         31                   0.964412\n",
       "32                         32                   0.964410\n",
       "33                         33                   0.964410\n",
       "34                         34                   0.964408\n",
       "35                         35                   0.964411\n",
       "36                         36                   0.964411\n",
       "37                         37                   0.964388\n",
       "38                         38                   0.964410\n",
       "39                         39                   0.964401\n",
       "40                         40                   0.964402\n",
       "41                         41                   0.964411\n",
       "42                         42                   0.964411\n",
       "43                         43                   0.964410\n",
       "44                         44                   0.964408\n",
       "45                         45                   0.964404\n",
       "46                         46                   0.964407\n",
       "47                         47                   0.964410\n",
       "48                         48                   0.962471\n",
       "49                         49                   0.962316"
      ]
     },
     "execution_count": 672,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sens_example = py_utils.sensitivity_analysis(X_train, X_test, y_test, logreg, 1.5, sklearn.metrics.roc_auc_score)\n",
    "sens_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that the scores are quite stable. <br>\n",
    "A distribution is available below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 1.0, 'Sensitivity distribution')]"
      ]
     },
     "execution_count": 673,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(sens_example.iloc[:,-1]).set(title = 'Sensitivity distribution')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Default]",
   "language": "python",
   "name": "conda-env-Default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
